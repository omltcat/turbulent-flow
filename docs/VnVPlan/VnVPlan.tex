\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{tabto}
\usepackage{amsmath}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Simulating Turbulent Flow with Synthetic Eddy: System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2024-02-19 & 1.0 & Initial plan after SRS\\
2024-03-22 & 1.1 & Address issues from feedback\\
2024-04-15 & 1.2 & Add unit tests\\
\bottomrule
\end{tabularx}

% ~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation technique
% that has ever been devised.  The VnV plan should also be a \textbf{feasible}
% plan. Execution of the plan should be possible with the time and team available.
% If the full plan cannot be completed during the time available, it can either be
% modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
% the design stage.  This means that the sections related to unit testing cannot
% initially be completed.  The sections will be filled in after the design stage
% is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

\listoftables
% \wss{Remove this section if it isn't needed}

\listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

Please refer to the Software Requirements Specification (\href{https://github.com/omltcat/turbulent-flow/blob/main/docs/SRS/SRS.pdf}{SRS}) of this project. Section 1 contains Symbols, Abbreviations, and Acronyms. If you are not familiar with fluids and CFD, Section 4.1.1 of the SRS contains a list of Terminology and Definitions.\\
~\newline
The following symbols and operators are used in this document:\\
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  tol & Tolerance, a small value used when testing differences, see \ref{secSymParams}\\
  std() & Standard deviation of multiple values.\\
  div() & Divergence of a velocity field.\\
  \bottomrule
\end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

\section{General Information}

This document lays the plan for the verification and validation of the software \progname{}. It provides an overview of what the tests aim to achieve, and then details the plan for each test. It also serves as a communication aid between the software development team and the domain experts on the theoretical side, so that it can be ensured that the correct aspect of the software is being tested with valid expectations.
% ... \wss{provide an introductory blurb and roadmap of the
  % Verification and Validation plan}

\subsection{Summary}

The software being tested is a program that generates approximated turbulent flow field with synthetic eddies. It is intended to provided initial and boundary conditions (IC and BC) to CFD solvers for turbulent flow simulations, which would cut down simulation time and save computational resources. An illustration of the flow field that the software is intended to generate is shown in Figure \ref{Fig_PhysicalSystem}.

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.8\textwidth]{PS.png}
  \caption{System Context}
  \label{Fig_PhysicalSystem} 
  \end{center}
\end{figure}


% \wss{Say what software is being tested.  Give its name and a brief overview of
%   its general functions.}

\subsection{Objectives}

The most crucial objective here is to build confidence that this software can correctly generate turbulent flow fields according to the synthetic eddy method proposed by \cite{PolettoEtAl2013}. As no suitable pseudo-oracle exists to allow for direct comparison of the results, the focus is to test for characteristics that a flow field generated with such method should exhibit. In the future with more theoretical background given, this will expand to making sure the generated flow fields are physically realistic in terms of turbulent properties. 

Another objective is to demonstrate the this software can interface with CFD solvers and provide to IC and BC for turbulent flow simulations (future development).

Furthermore, maintainability, namely the ability for user to easily modify the software to fit their specific needs should also be demonstrated.

% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}

\subsection{Relevant Documentation}

\begin{itemize}
  \item Software Requirements Specification (\href{https://github.com/omltcat/turbulent-flow/blob/main/docs/SRS/SRS.pdf}{SRS}) is the most important guideline for this VnV plan. It contains the requirements of the software, and the theoretical Modules of the synthetic eddy method. 
  \item Design Documents will be used to guide the unit testing and the design verification. This include the Module Guide (\href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}) and the Module Interface Specification (\href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}). 
\end{itemize}

% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.}

% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

\section{Plan}

This section lays out how the VnV will be carried out in aspect and by whom. It also includes the tools that will be used for the VnV.

% \wss{Introduce this section.   You can provide a roadmap of the sections to
%   come.}

\subsection{Verification and Validation Team}

\begin{itemize}
  \item Phil Du - Developer: Writing test scripts according to VnV and implementing automated tests with continuous integration (CI). Ensuring coding standards are respected with linters.
  \item Nikita Holyev - Theorist: Provide theoretical background support and potential usage cases for the software.
  \item Dr. Spencer Smith - Supervisor. Reviewing all documents.
  \item Cynthia Liu - Domain Expert: Reviewing all documents.
  \item Kim Ying Wong - Secondary reviewer of SRS.
  \item Morteza Mirzaei - Secondary reviewer of VnV Plan.
  \item Nada Elmasry - Secondary reviewer of MG and MIS.
  \item Several volunteers as subjects for usability (future) and maintainability testing, see \ref{NonfuncTest} for detailed procedures. They are recruited from class as well as potential users of the software.
\end{itemize}

% \wss{Your teammates.  Maybe your supervisor.
%   You should do more than list names.  You should say what each person's role is
%   for the project's verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}

The (\href{https://github.com/omltcat/turbulent-flow/blob/main/docs/SRS/SRS.pdf}{SRS}) is reviewed by Cynthia Liu, Kim Ying Wong and Dr. Spencer Smith to ensure that the requirements are clear with good traceability, and the models are represented to be easily understandable. The \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS Checklist} can be used as reference. Feedback will be given as GitHub Issues, and the developer will update the document accordingly.

The developer will also go-through the models in the SRS with Nikita Holyev to ensure that they are mathematically correct and reflect the current direction of work in the theoretical side.

% \wss{List any approaches you intend to use for SRS verification.  This may include
%   ad hoc feedback from reviewers, like your classmates, or you may plan for 
  % something more rigorous/systematic.}

% \wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}

The design documents, including the Module Guide (\href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}) and the Module Interface Specification (\href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}), is reviewed by Cynthia Liu, Nada Elmasry and Dr. Spencer Smith to ensure that the design is unambiguous, inline with module design best-practices and consistent with the requirements in the SRS. The \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Checklists/MG-Checklist.pdf}{MG Checklist} and \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Checklists/MIS-Checklist.pdf}{MIS Checklist} can be used as references. Feedback will be given as GitHub Issues, and the developer will update the documents accordingly.

Discussions will also be held with Nikita Holyev to identify potential usage cases that may require future proofing in the design.

% \textit{Pending design documents.}

% \wss{Plans for design verification}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

The VnV plan is reviewed by Cynthia Liu, Morteza Mirzaei and Dr. Spencer Smith to ensure the plan reflects requirements in the SRS with clear and specific test cases. The \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Checklists/VnV-Checklist.pdf}{VnV Checklist} can be used as reference. Feedback will be given as GitHub Issues, and the developer will update the document accordingly.
 

% \wss{The verification and validation plan is an artifact that should also be
% verified.  Techniques for this include review and mutation testing.}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Implementation Verification Plan}

Successful implementation of the synthetic eddy method can be verified by automated tests detailed in Section \ref{FuncTest}. These tests will be run during development on local machines, and before merging to the main branch with GitHub Actions.

To ensure the quality of the codebase, the \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Checklists/Code-Checklist.pdf}{Source Code Checklist} provided by Dr. Smith will be used as a guideline throughout the development process. 

% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}

% \wss{In this section you would also give any details of any plans for static
%   verification of the implementation.  Potential techniques include code
%   walkthroughs, code inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

\begin{itemize}
  \item Continuous Integration (CI): GitHub Actions to run automated tests upon pull request to main branch.
  \item System and Unit Tests: \href{https://docs.pytest.org/}{pytest}.
  \item Code Coverage: \href{https://docs.pytest.org/}{pytest}.
  \item Linters: \href{https://marketplace.visualstudio.com/items?itemName=ms-python.flake8}{flake8}: ensure PEP8 coding standards are respected.
\end{itemize}

% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}

% \wss{If you have already done this in the development plan, you can point to
% that document.}

% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}

\subsection{Software Validation Plan}

Validation against real-life experiments is outside the scope of this project. The basis of \progname{} is theoretical. The focus is to ensure that the software properly represent the theoretical proposals [SRS: TM1, TM2 and GD1].

% As there is no suitable external data or pseudo-oracle to directly compare with the results of this software, the validation will be done by comparing the generated flow fields with the expected theoretical characteristics of the synthetic eddy method. Several aspects of the flow field will be tested, as detailed in Section \ref{FuncTest}. The test scripts will be run automatically with GitHub Actions.

% \wss{If there is any external data that can be used for validation, you should
%   point to it here.  If there are no plans for validation, you should state that
%   here.}

% \wss{You might want to use review sessions with the stakeholder to check that
% the requirements document captures the right requirements.  Maybe task based
% inspection?}

% \wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
% be used as an opportunity to validate the requirements.  You should plan on 
% demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
% The feedback from your supervisor will be very useful for improving your project.}

% \wss{For teams without an external supervisor, user testing can serve the same purpose 
% as a Rev 0 demo for the supervisor.}

% \wss{This section might reference back to the SRS verification section.}
\newpage
\section{System Test Description}

The system tests are divided into two main categories: Functional and Nonfunctional Requirements. The Functional Requirements, other than the CFD interface, are tested with automated scripts, while the Nonfunctional Requirements are tested manually. The tests are traced to the requirements in the \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/SRS/SRS.pdf}{SRS}.
	
\subsection{Tests for Functional Requirements} \label{FuncTest}

These tests are traced to R1-R3 (Section 5.1) of the \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/SRS/SRS.pdf}{SRS}. Only test for R3 is manual, as it involves interfacing with CFD software. The rest are automated with pytest and GitHub Actions.

Areas of testing are structured with respect to the list of Function Requirements.

% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.}

\subsubsection{Input Test} \label{ST:Input}


% \wss{It would be nice to have a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.  If a section
%   covers tests for input constraints, you should reference the data constraints
%   table in the SRS.}
		
\paragraph{Check Query Point within Flow Field [R1]}

\begin{enumerate}

\item{test-query-coordinate\\}

Control: Automatic
					
Initial State: Empty flow field with size (1m, 1m, 1m)
					
Input: $\mathbf{x}=(2,2,2)$
					
Output: Throw exception: Queried point outside of flow field!

Test Case Derivation: Execution should be halted as results outside of the flow field are not meaningful.
					
How test will be performed: Automated test script, GitHub Actions.

\end{enumerate}

\newpage
\subsubsection{Eddy Generation Test} \label{ST:EddyGen}
\paragraph{Generation of a Single Eddy [R2]}

\begin{enumerate}

  \item{test-eddy-shape\\}

  Control: Automatic
            
  Initial State: Empty flow field, zero $\mathbf{u_{avg}}$
            
  Input:
  \begin{itemize}
    \item Profile: 1 eddy with predetermined center location\\
    $\mathbf{x}=(0,0,0)$\tab$\sigma=1$ (length-scale, i.e. radius) 
    \item Query: Pairs of points relative to center:\\
    $\mathbf{x_1}=(0.5,0.5,0.5)$ \tab$\mathbf{x_2}=(-0.5,-0.5,-0.5)$\\
    $\mathbf{x_3}=(2,2,2)$ \tab$\mathbf{x_4}=(-2,-2,-2)$
  \end{itemize}
  Output: Velocity vectors at each point: $\mathbf{u_1}$, $\mathbf{u_2}$, $\mathbf{u_3}$, $\mathbf{u_4}$\\
  Expected behavior:
  \begin{itemize}
    \item $|\mathbf{u_1} -\mathbf{u_2}| <\text{tol}|\mathbf{u_1}|$\tab $|\mathbf{u_3}| = |\mathbf{u_4}|=0$
  \end{itemize}

  Test Case Derivation: Points should have equal and opposite velocities inside the eddy, and zero velocity outside. This is a result of the shape function cutting off the influence of each eddy beyond $\sigma$.
            
  How test will be performed: Automated test script, GitHub Actions.

  \begin{figure}[h!]
    \begin{center}
    \includegraphics[width=0.3\textwidth]{eddy-shape.png}
    \caption{Query Pair of Points}
    \label{Fig_EddyShape} 
    \end{center}
  \end{figure}

  \newpage
  \item{test-eddy-std\\}

  Control: Automatic
            
  Initial State: Empty flow field, zero $\mathbf{u_{avg}}$
            
  Input:
  \begin{itemize}
    \item Profile: 1 eddy with predetermined orientation\\
    $\mathbf{x}=(0,0,0)$\tab$\sigma=1$\\
    $\alpha=(0,0,1)$ (intensity = 1, orientation along z-axis)
    \item Query: Mesh grid of points covering the eddy
  \end{itemize}
  Output: Velocity vectors at each point $\mathbf{u}$\\
  Expected behavior:
  \begin{itemize}
    \item $\text{std}(\mathbf{u_z}) < \text{tol}$
  \end{itemize}

  Test Case Derivation: Flow of an eddy should be around its orientation axis. There should not be velocity in the axial direction. Thus, velocity standard deviation along the eddy orientation should be zero (below tolerance).
            
  How test will be performed: Automated test script, GitHub Actions.

  \begin{figure}[h!]
    \begin{center}
    \includegraphics[width=0.3\textwidth]{eddy-orient.png}
    \caption{Eddy Orientation}
    \label{Fig_EddyOrient} 
    \end{center}
  \end{figure}

\end{enumerate}

\newpage
\subsubsection{Field Genration Test} \label{ST:FieldGen}
\paragraph{Generation of Flow Field [R2]}

\begin{enumerate}

  \item{test-field-mean\\}

  Control: Automatic
            
  Initial State: Empty flow field, $\mathbf{u_{avg}}$
            
  Input:
  \begin{itemize}
    \item Profile: Any number of eddies, random orientations
    \item Query: Mesh grid of points of the entire flow field at any $t$
  \end{itemize}
  Output: Velocity vectors at each point: $\mathbf{u}$\\
  Expected behavior:
  \begin{itemize}
    \item $|\frac{\sum\mathbf{u}-\mathbf{u_{avg}}}{N_{grid~points}}| < \text{tol}$
  \end{itemize}

  Test Case Derivation: The vector mean of all velocity fluctuations caused by the eddies should be zero (below tolerance). For eddies completely within the field, this should be inherently true as velocity moves in circles. For eddies partially outside the field boundary, they should be wrapped around to the opposite side of the field to ensure this property.
            
  How test will be performed: Automated test script, GitHub Actions.

  \item{test-field-std-time\\}

  Control: Automatic
            
  Initial State: Empty flow field, $\mathbf{u_{avg}}$
            
  Input:
  \begin{itemize}
    \item Profile: Any number of eddies, random orientations
    \item Query: Mesh grid of points of the entire flow field with \\ $t_0=0$ and $t_1=10$.
  \end{itemize}
  Output: Velocity vectors at each point: $\mathbf{u}$\\
  Expected behavior:
  \begin{itemize}
    \item $\text{std}(\mathbf{u})|_{t_0} - \text{std}(\mathbf{u})|_{t_1} < \text{tol}$
  \end{itemize}

  Test Case Derivation: Velocity standard deviation should stay the same regardless of time, as the eddies are only moved to different locations in the field.

  How test will be performed: Automated test script, GitHub Actions.

  \item{test-field-std-intensity\\}

  Control: Automatic
            
  Initial State: Empty flow field, $\mathbf{u_{avg}}$
            
  Input:
  \begin{itemize}
    \item Profile: Any number of eddies, random orientations\\
    Increasing eddy intensity $|\mathbf{\alpha}|$
    \item Query: Mesh grid of points of the entire flow field
  \end{itemize}
  Output: Velocity vectors at each point: $\mathbf{u}$\\
  Expected behavior:
  \begin{itemize}
    \item if $|\mathbf{\alpha}|_1 > |\mathbf{\alpha}|_2$ then $\text{std}(\mathbf{u})_1 > \text{std}(\mathbf{u})_2$
  \end{itemize}

  Test Case Derivation: Velocity standard deviation should increase with eddy intensity $|\mathbf{\alpha}|$, as the velocity fluctuations are stronger with higher intensity.

  How test will be performed: Automated test script, GitHub Actions.

\end{enumerate}

\subsubsection{Divergence Test} \label{ST:DivFree}
\paragraph{Divergence Free Condition [R2]} 

\begin{enumerate}

  \item{test-field-divergence\\}

  Control: Automatic
            
  Initial State: Empty flow field, $\mathbf{u_{avg}}$
            
  Input:
  \begin{itemize}
    \item Profile: Any number of eddies, random orientations
    \item Query: Mesh grid of points of the entire flow field
  \end{itemize}
  Output: Velocity vectors at each point: $\mathbf{u}$\\
  Expected behavior:
  \begin{itemize}
    \item $\text{div}(\mathbf{u}) < \text{tol}$
  \end{itemize}

  Test Case Derivation: The flow field should be divergence-free (below tolerance). Divergence measures if a vector field satisfies the conservation of mass, i.e. nothing is created or destroyed. See \href{https://numpy.org/doc/stable/reference/generated/numpy.gradient.html}{numpy.gradient} for more information regarding its calculation.

  How test will be performed: Automated test script, GitHub Actions.

\end{enumerate}

\subsubsection{CFD Interface} \label{ST:CFD}
\paragraph{Feed IC and BC to CFD Software [R3]\\}

\begin{enumerate}

  \item{test-cfd-interface\\}
  
  Type: Manual.
            
  Initial State: Empty flow field, $\mathbf{u_{avg}}$
            
  Input:
  \begin{itemize}
    \item Profile: Any number of eddies, random orientations
    \item Query: CFD software should query points in the flow field
  \end{itemize}
  Output: Velocity vectors at each point: $\mathbf{u}$\\
  Expected behavior:
  \begin{itemize}
    \item They should be feed into the CFD software
  \end{itemize}

  Test Case Derivation: The CFD software should be able get the flow field generated by \progname{} as IC and BC.

  How test will be performed: Manually perform a CFD simulation in conjecture with  \progname{}.
\end{enumerate}

\newpage
\subsection{Tests for Nonfunctional Requirements} \label{NonfuncTest}
NFR2 is already covered by functional test of CFD interface (\ref{ST:CFD}). NFR1 (accuracy), NFR3 (maintainability) NFR4(portability) and NFR5 (performance) will be verified by tests detailed below.
% \wss{The nonfunctional requirements for accuracy will likely just reference the
%   appropriate functional tests from above.  The test cases should mention
%   reporting the relative error for these tests.  Not all projects will
%   necessarily have nonfunctional requirements related to accuracy}

% \wss{Tests related to usability could include conducting a usability test and
%   survey.  The survey will be in the Appendix.}

% \wss{Static tests, review, inspections, and walkthroughs, will not follow the
% format for the tests given below.}

\subsubsection{Accuracy} \label{ST:EddyProf}
\paragraph{Provide Realistic Profile Before Generation \\}
Test case for this requirement will be added in the future given more theoretical background.

\subsubsection{Maintainability} \label{ST:Maint}
		
\paragraph{Modifying Shape Function\\}
In terms of maintainability, users who looks to modify \progname{} would most likely want to change the shape functions of the eddies to fit their specific needs. If the software is well structured and documented, they should be able to do so with ease, even without systematic programming knowledge.

The test subjects are not expected to be familiar with the theoretical background of the synthetic eddy method, as they will be given a valid mathematical equation of the shape function. They should have at least some basic programming exposures, such as using MATLAB or Python for simple calculations or data processing.

\begin{enumerate}

\item{test-modify-shape-func\\}

Type: Static
					
Initial State: Reviewer is provided with documentation and source code, and no other help.
					
Input/Condition: Reviewer is given a mathematical equation for a new shape function.
					
Output/Result: Reviewer should be able to modify the source code to implement the new shape function.
					
How test will be performed: Performed by volunteers or potential users of the software. Time taken will be recorded. Check if the modified software still passes the automated functional tests.

\end{enumerate}

\subsubsection{Portability} \label{ST:Port}
\paragraph{Cross-platform Test\\}
Since unit tests and system tests executed by GitHub Actions are performed on Ubuntu runners (\href{https://github.com/omltcat/turbulent-flow/blob/main/.github/workflows/run-tests.yaml}{see here}), this already covers installing and running on Linux. 

To test Windows and MacOS, an additional GitHub Action workflow is created to install and run a test of the Main Control Module on Windows and MacOS runners (\href{https://github.com/omltcat/turbulent-flow/blob/main/.github/workflows/cross-platform.yaml}{see here}). The details of this test file can be found in Section \ref{UT:Main}. The only reason for choosing this specific test is that it is fast to perform, as MacOS runners are billed 10 times as expensive as Linux runners.

\subsubsection{Performance} \label{ST:Perf}
\paragraph{Computing Time and Memory Usage\\}
Test case in this section mimics real-life usage. The test is not meant to execute until finish as it would take too long. Instead, it is used to give a rough estimate of the computing time and memory usage.

\begin{enumerate}

\item{test-performance\\}

Type: Manual

Initial State: \progname{} is installed and ready to run.

Input: 
\begin{itemize}
  \item Field: $20\times20\times20$ field with around 10 million eddies
  \item Query: $1000\times1000\times1000$ meshgrid
\end{itemize}

Output: Memory usage and estimated computing time

How test will be performed: Developer will run the test on their local machine while monitor task manager and command line output. The program will give an estimate of the computing time as it computes the velocities in the field. The memory usage by the program reported by task manager will be taken when the program is running.

\end{enumerate}

\newpage
\subsection{Traceability Between Test Cases and Requirements}
\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
                            & R1 & R2 & R3 & NFR1 & NFR2 & NFR3 & NFR4 & NFR5 \\
  \hline
  Test \ref{ST:Input}     & X  &    &    &      &      &      &      &      \\\hline
  Test \ref{ST:EddyGen}   &    & X  &    &      &      &      &      &      \\\hline
  Test \ref{ST:FieldGen}  &    & X  &    &      &      &      &      &      \\\hline
  Test \ref{ST:DivFree}   &    & X  &    &      &      &      &      &      \\\hline
  Test \ref{ST:CFD}       &    &    & X  &      & X    &      &      &      \\\hline
  Test \ref{ST:EddyProf}  &    &    &    & X    &      &      &      &      \\\hline
  Test \ref{ST:Maint}     &    &    &    &      &      & X    &      &      \\\hline
  Test \ref{ST:Port}      &    &    &    &      &      &      & X    &      \\\hline
  Test \ref{ST:Perf}      &    &    &    &      &      &      &      & X    \\\hline
  \end{tabular}
  \caption{Traceability Between Test Cases and Requirements}
  \label{Table:A_trace}
\end{table}

% \wss{Provide a table that shows which test cases are supporting which
%   requirements.}

\section{Unit Test Description}

Unit tests are written for the modules described in the \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG} and \href{https://github.com/omltcat/turbulent-flow/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}. For most modules, a dedicated test file is created, unless that module cannot function independently and can be sufficiently tested when other modules are tested.

Each module is tested for the expected use cases, edge cases and exceptions. These tests are heavily leveraged during development to ensure that changes in one module do not break the functionality of another.

All unit tests that are not designated as ``slow'' in the test file are run by GitHub Actions upon pull request to the main branch.

% \wss{This section should not be filled in until after the MIS (detailed design
%   document) has been completed.}

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}  

% \wss{To save space and time, it may be an option to provide less detail in this section.  
% For the unit tests you can potentially layout your testing strategy here.  That is, you 
% can explain how tests will be selected for each module.  For instance, your test building 
% approach could be test cases for each access program, including one test for normal behaviour 
% and as many tests as needed for edge cases.  Rather than create the details of the input 
% and output here, you could point to the unit testing code.  For this to work, you code 
% needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

Unit tests are performed on all modules developed for \progname{}, except:
\begin{itemize}
  \item Eddy Module [MG: M6]: The inputs to this module need to be generated by the Flow Field Module. Thus, the Eddy Module will be tested when the Flow Field Module is tested.
  \item Visualization Module [MG: M10]: This module is only a placeholder at current stage. However, it is still covered by the test cases in the Query Interface Module.
  \item Utils: Not actually a module in the design, but several helper functions to reduce code repetition. These functions are tested when the modules that use them are tested.
\end{itemize} 

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

\subsubsection{Main Control Module [MG: M2]} \label{UT:Main}

Test for creating a new field and query an existing field, setting active shape functions and cutoff, as well as handling exceptions raised by lower-level modules. See \href{https://github.com/omltcat/turbulent-flow/blob/main/test/test_main.py}{test\_main.py}


% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

\begin{enumerate}
  \item test-main-new
  \item test-main-query
  \item test-main-new-exception
  \item test-main-query-field-not-exist
  \item test-main-query-shape-function-exceptions
  \item test-main-query-cutoff
  \item test-main-query-exception

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 
					
% \item{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input: 
					
% Output: \wss{The expected result for the given inputs}

% Test Case Derivation: \wss{Justify the expected value given in the Output field}

% How test will be performed: 

% \item{...\\}
    
\end{enumerate}

\subsubsection{Query Interface Module [MG: M3]} \label{UT:Query}
Test for query in meshgrid and points modes, as well as handling exceptions when the query is invalid. See \href{https://github.com/omltcat/turbulent-flow/blob/main/test/test_query.py}{test\_query.py}

\begin{enumerate}
  \item test-query-meshgrid
  \item test-query-points
  \item test-query-meshgrid-exceptions
  \item test-query-points-exceptions
  \item test-query-mode-exceptions
  \item test-query-plot-exceptions
\end{enumerate}

\subsubsection{Eddy Profile Module [MG: M4]} \label{UT:Prof}
Test for loading valid and invalid eddy profiles (exceptions). See \href{https://github.com/omltcat/turbulent-flow/blob/main/test/test_eddy_profile.py}{test\_eddy\_profile.py}

\begin{enumerate}
  \item test-eddy-profile
  \item test-eddy-profile-invalid
\end{enumerate}

\subsubsection{Flow Field Module [MG: M5]} \label{UT:Field}
Test initializing field and calculating velocities with meshgrid. Eddy generation is also tested here. Exceptions during initialization and calculation are also tested. See \href{https://github.com/omltcat/turbulent-flow/blob/main/test/test_flow_field.py}{test\_flow\_field.py}

\begin{enumerate}
  \item test-eddy-generation
  \item test-flow-field
  \item test-flow-field-wrap
  \item test-flow-field-init-exceptions
  \item test-flow-field-mesh-exceptions
  \item test-flow-field-set-exceptions
  \item test-flow-field-out-of-memory
\end{enumerate}

\subsubsection{Shape Function Module [MG: M7]} \label{UT:Shape}
Test for calculating shape function values with different active shape functions and cutoffs, as well as exceptions when setting active shape function or cutoff. See \href{https://github.com/omltcat/turbulent-flow/blob/main/test/test_shape_function.py}{test\_shape\_function.py}

\begin{enumerate}
  \item test-shape-function
  \item test-shape-function-exceptions
\end{enumerate}

\subsubsection{File IO Module [MG: M8]} \label{UT:FileIO}
Test for reading, writing and deleting files within the program directory, as well as handling exceptions when the file is not found, unable to read/write or the expected format is not matched. See \href{https://github.com/omltcat/turbulent-flow/blob/main/test/test_file_io.py}{test\_file\_io.py}

\begin{enumerate}
  \item test-file-io
  \item test-file-io-read-fail
  \item test-file-io-read-fail-format
  \item test-file-io-write-fail
  \item test-file-io-clear-fail
\end{enumerate}


% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module ?}
		
% \begin{enumerate}

% \item{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% Initial State: 
					
% Input/Condition: 
					
% Output/Result: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsubsection{Module ?}


\subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
                            & M2 & M3 & M4 & M5 & M6 & M7 & M8 & M10 \\
  \hline
  Test \ref{UT:Main}        & X  &    &    &    &    &    &    &     \\\hline
  Test \ref{UT:Query}       &    & X  &    &    &    &    &    & X   \\\hline
  Test \ref{UT:Prof}        &    &    & X  &    &    &    &    &     \\\hline
  Test \ref{UT:Field}       &    &    &    & X  & X  &    &    &     \\\hline
  Test \ref{UT:Shape}       &    &    &    &    &    & X  &    &     \\\hline
  Test \ref{UT:FileIO}      &    &    &    &    &    &    & X  &     \\\hline
  \end{tabular}
  \caption{Traceability Between Unit Test Cases and Modules}
  \label{Table:A_trace}
\end{table}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

% This is where you can place additional information.

\subsection{Symbolic Parameters} \label{secSymParams}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\begin{itemize}
  \item Tolerance: some small value, e.g. $\text{tol}=0.0001$
  \item Average velocity: along x-axis (1D flow), e.g. $\mathbf{u_{avg}}=(5,0,0)$
\end{itemize}


% \subsection{Usability Survey Questions?}

% \wss{This is a section that would be appropriate for some projects.}

% \newpage{}
% \section*{Appendix --- Reflection}

% The information in this section will be used to evaluate the team members on the
% graduate attribute of Lifelong Learning.  Please answer the following questions:

% \newpage{}
% \section*{Appendix --- Reflection}

% \wss{This section is not required for CAS 741}

% The information in this section will be used to evaluate the team members on the
% graduate attribute of Lifelong Learning.  Please answer the following questions:

% \begin{enumerate}
%   \item What knowledge and skills will the team collectively need to acquire to
%   successfully complete the verification and validation of your project?
%   Examples of possible knowledge and skills include dynamic testing knowledge,
%   static testing knowledge, specific tool usage etc.  You should look to
%   identify at least one item for each team member.
%   \item For each of the knowledge areas and skills identified in the previous
%   question, what are at least two approaches to acquiring the knowledge or
%   mastering the skill?  Of the identified approaches, which will each team
%   member pursue, and why did they make this choice?
% \end{enumerate}

\end{document}